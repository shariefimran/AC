{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OverFitting UnderFitting\n",
    "\n",
    "OverFitting -> On Trainig data. Classifier try to fit function / decision surface to have no error.\n",
    "\n",
    "Sensitive to outliers\n",
    "\n",
    "\n",
    "UnderFitting -> Classifier is lazy\n",
    "\n",
    "# How to Determine the right K? \n",
    "\n",
    "Cross Validation -> Train data and using different k test on test data\n",
    "\n",
    "Generalization : Algo performs well on the future unseen points\n",
    "\n",
    "100% Data-> 60% Training ,Cross validation -> 20% , 20% -> Test Data\n",
    "\n",
    "K Cross Validation\n",
    "\n",
    "Problem with Cross Validation -> Losing 40 % data\n",
    "\n",
    "Different apporcha-> Take 80% data and take k chunks of data\n",
    "\n",
    "How to determine that we are not underfitting and overfitting\n",
    "\n",
    "Accuary and Error\n",
    "\n",
    "\n",
    "Train Error and Cross Validation Error are higher then underfit\n",
    "\n",
    "Train Error is less and Cross Validation Error is high then overfit\n",
    "\n",
    "\n",
    "# Time Based Splitting\n",
    "\n",
    "\n",
    "There should be a time column and oldest 60% as train \n",
    "\n",
    "Time is a necesssary column\n",
    "\n",
    "\n",
    "# KNN for regression\n",
    "GET k nn values and get  mean or median\n",
    "\n",
    "# Weighted KNN algorithm\n",
    "\n",
    "weight = 1/distance of the point. Irrespective of max number based on the weight to distance whic are close\n",
    "\n",
    "# vornoi diagram\n",
    "Region Based\n",
    "\n",
    "# Binary Search Tree\n",
    "\n",
    "Used to search the element Time-> O(log(n))\n",
    "\n",
    "# Build KD tree\n",
    "\n",
    "projcet data to axis(2d assume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-NN Cross validation \n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('C:\\\\Users\\\\Mekakris\\\\Documents\\\\AC\\\\amazon-fine-food-reviews-data\\\\database.sqlite')\n",
    "\n",
    "data = pd.read_sql_query(\"select * from Reviews where score!=3\",con)\n",
    "\n",
    "data=data[:3000]\n",
    "\n",
    "\n",
    "def parition(x):\n",
    "    if x<3:\n",
    "        return \"negative\"\n",
    "    return \"positive\"\n",
    "\n",
    "actualscore=data['Score']\n",
    "\n",
    "\n",
    "positiveNegative=actualscore.map(parition)\n",
    "data['Score']=positiveNegative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2991, 10)\n",
      "(588, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    343\n",
       "negative    245\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data De duplication \n",
    "\n",
    "data=data.sort_values('ProductId',axis=0,ascending=True)\n",
    "data=data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep=\"first\",inplace=False)\n",
    "print(data.shape)\n",
    "\n",
    "data=data[data.HelpfulnessNumerator<data.HelpfulnessDenominator]\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data['Score'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text \n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\\n\",\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        cleantext = re.sub(cleanr, ' ', sentence)\n",
    "        return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\\n\",\n",
    "        cleaned = re.sub(r'[?|!|\\'|\\\"|#]',r'',sentence)\n",
    "        cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "        return  cleaned\n",
    "\n",
    "#print(stop)\n",
    "\n",
    "\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\\n\",\n",
    "all_negative_words=[] # store words from -ve reviews here.\\n\",\n",
    "s=''\n",
    "\n",
    "#data=data.iloc[0:1000]\n",
    "\n",
    "\n",
    "for sent in data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\\n\",\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\\n\",\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (data['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\\n\",\n",
    "                    if(data['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\\n\",\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "               continue \n",
    "    #print(filtered_sentence)\\n\",\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\\n\n",
    "    #print(\\\"***********************************************************************\\\")\\n\",\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "\n",
    "    #rint(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['CleanedText']=final_string\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "#returns the object and convets text to vector\n",
    "#count_vect = CountVectorizer() -> this is for normal text vector\n",
    "#count_vect = CountVectorizer(ngram_range=(1,3))# -> this is for normal text vector\n",
    "#final=count_vect.fit_transform(data['CleanedText'].values)\n",
    "\n",
    "#print(final.shape)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sort Data using time series\n",
    "\n",
    "data = data.sort_values(['Time'])\n",
    "lables=data['Score']\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1,2))# -> this is for normal text vector\n",
    "datavector=count_vect.fit_transform(data['CleanedText'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 23759) (314,)\n",
      "(79, 23759) (79,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mekakris\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# simple train test split\n",
    "x_1, x_test, y_1, y_test = cross_validation.train_test_split( datavector, lables, test_size=0.33, random_state=42)\n",
    "# x_1 has data. y_1 has train data lables\n",
    "\n",
    "x_train,x_cv,y_train,y_cv = cross_validation.train_test_split(x_1,y_1,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "#print(train_vector.shape)\n",
    "#print(validation_vector.shape)\n",
    "#print(y_cv.shape)\n",
    "#print(type(train_vector))\n",
    "#print(neigh.predict(x_test[5]))\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_cv.shape,y_cv.shape)\n",
    "#print(y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For k accuracy is %d 1 48.10126582278481\n",
      "For k accuracy is %d 3 48.10126582278481\n",
      "For k accuracy is %d 5 48.10126582278481\n",
      "For k accuracy is %d 7 48.10126582278481\n",
      "For k accuracy is %d 9 48.10126582278481\n",
      "For k accuracy is %d 11 48.10126582278481\n",
      "For k accuracy is %d 13 48.10126582278481\n",
      "For k accuracy is %d 15 48.10126582278481\n",
      "For k accuracy is %d 17 58.22784810126582\n",
      "For k accuracy is %d 19 50.63291139240506\n",
      "For k accuracy is %d 21 49.36708860759494\n",
      "For k accuracy is %d 23 48.10126582278481\n",
      "For k accuracy is %d 25 49.36708860759494\n",
      "For k accuracy is %d 27 48.10126582278481\n",
      "For k accuracy is %d 29 48.10126582278481\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "standardized_train_data = StandardScaler(with_mean=False).fit_transform(x_train)\n",
    "standardized_test_data = StandardScaler(with_mean=False).fit_transform(x_cv)\n",
    "\n",
    "#print(type(standardized_train_data))\n",
    "#print(standardized_test_data.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_cv.shape)\n",
    "for i in range(1,30,2):\n",
    "    \n",
    " knn = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    # Train the the model\n",
    " knn.fit(standardized_train_data,y_train)\n",
    "    \n",
    "    # Predict\n",
    " pred = knn.predict(standardized_test_data)\n",
    "    \n",
    "    #Evaluate the accuracy\n",
    " acc=accuracy_score(y_cv,pred,normalize=True) * float(100)\n",
    "    \n",
    " print('For k accuracy is %d',i,acc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 23759) (79, 23759)\n",
      "Number of mislabeled points out of a total %d points : %d (314, 28)\n"
     ]
    }
   ],
   "source": [
    "# NAIVE BAYES\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "standardized_train_data = StandardScaler(with_mean=False).fit_transform(x_train)\n",
    "standardized_test_data = StandardScaler(with_mean=False).fit_transform(x_cv)\n",
    "\n",
    "print(standardized_train_data.shape,standardized_test_data.shape)\n",
    "\n",
    "#print(type(standardized_train_data))\n",
    "#print(standardized_test_data.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_cv.shape)\n",
    "gnb = GaussianNB()\n",
    "    # Train the the model\n",
    "\n",
    "y_pred = gnb.fit(standardized_train_data.toarray(), y_train).predict(standardized_test_data.toarray())\n",
    "    \n",
    "print(\"Number of mislabeled points out of a total %d points : %d\",(standardized_train_data.shape[0],(y_cv != y_pred).sum()))\n",
    "\n",
    "\n",
    "\n",
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 23759) (79, 23759)\n",
      "Number of mislabeled points out of a total %d points : %d (314, 79)\n"
     ]
    }
   ],
   "source": [
    "# K-Means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "standardized_train_data = StandardScaler(with_mean=False).fit_transform(x_train)\n",
    "standardized_test_data = StandardScaler(with_mean=False).fit_transform(x_cv)\n",
    "\n",
    "print(standardized_train_data.shape,standardized_test_data.shape)\n",
    "\n",
    "#print(type(standardized_train_data))\n",
    "#print(standardized_test_data.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_cv.shape)\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(standardized_train_data.toarray(),y_train)\n",
    "\n",
    "y_pred =kmeans.predict(standardized_test_data.toarray())\n",
    "    \n",
    "print(\"Number of mislabeled points out of a total %d points : %d\",(standardized_train_data.shape[0],(y_cv != y_pred).sum()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 23759) (79, 23759)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DBSCAN' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ae82fc5d0992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdbscan\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstandardized_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mdbscan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstandardized_test_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of mislabeled points out of a total %d points : %d\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstandardized_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_cv\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DBSCAN' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# DB-Scan-Means hold this. I need to work on this . this is incomplete\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "standardized_train_data = StandardScaler(with_mean=False).fit_transform(x_train)\n",
    "standardized_test_data = StandardScaler(with_mean=False).fit_transform(x_cv)\n",
    "\n",
    "print(standardized_train_data.shape,standardized_test_data.shape)\n",
    "\n",
    "#print(type(standardized_train_data))\n",
    "#print(standardized_test_data.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_cv.shape)\n",
    "#dbscan =  DBSCAN(eps=0.3, min_samples=10).fit(standardized_train_data.toarray(),y_train)\n",
    "\n",
    "#y_pred =dbscan.predict(standardized_test_data.toarray())\n",
    "    \n",
    "#print(\"Number of mislabeled points out of a total %d points : %d\",(standardized_train_data.shape[0],(y_cv != y_pred).sum()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(314, 23759) (79, 23759)\n",
      "Number of mislabeled points out of a total %d points : %d (314, 33)\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree hold this. I need to work on this . this is incomplete\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "\n",
    "standardized_train_data = StandardScaler(with_mean=False).fit_transform(x_train)\n",
    "standardized_test_data = StandardScaler(with_mean=False).fit_transform(x_cv)\n",
    "\n",
    "print(standardized_train_data.shape,standardized_test_data.shape)\n",
    "\n",
    "#print(type(standardized_train_data))\n",
    "#print(standardized_test_data.shape)\n",
    "#print(y_train.shape)\n",
    "#print(y_cv.shape)\n",
    "tree=tree.DecisionTreeClassifier().fit(standardized_train_data.toarray(),y_train)\n",
    "\n",
    "y_pred =tree.predict(standardized_test_data.toarray())\n",
    "    \n",
    "print(\"Number of mislabeled points out of a total %d points : %d\",(standardized_train_data.shape[0],(y_cv != y_pred).sum()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
